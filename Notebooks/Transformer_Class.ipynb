{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import glob\n",
    "import csv\n",
    "from spellchecker import SpellChecker\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn import *\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import gensim.models.wrappers.fasttext\n",
    "from scipy import sparse\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import LeaveOneOut,KFold,train_test_split\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "from sklearn.metrics import accuracy_score\n",
    "import simplejson\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from mr_generic_scripts import *\n",
    "from mr_cls_Transformer import *\n",
    "from mr_generic_scripts import load_combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to xlsx files folder\n",
    "\n",
    "# Original MIND-CA corpus\n",
    "path_to_raw_files = f_path + 'Data/raw_relabeled/'\n",
    "# MIND-CA + human augment\n",
    "path_to_plus_files = f_path + 'Data/raw_plus/'\n",
    "# UK-MIND-20\n",
    "path_to_wp1 = f_path + 'Data/wp1/'\n",
    "\n",
    "# Augmented data\n",
    "# augmentations, 125 examples per QA pair\n",
    "path_to_aug = f_path + 'Data/aug_data/all/'\n",
    "# augmentations, 500 examples per QA pair\n",
    "path_to_aug_hq_os = f_path + 'Data/aug_data_os/all/'\n",
    "# augmentation, no sampling - 1500 total examples per question\n",
    "path_to_aug_joint = f_path + 'Data/aug_data_joint/all/'\n",
    "\n",
    "# Merged xlsx files with multiple augmentations\n",
    "path_to_set_files = f_path + 'Data/aug_data/sets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of augmentations by category\n",
    "hq_data = ['reord','phrase','dict']\n",
    "lq_data = ['wordnet','ppdb','glove','fasttext']\n",
    "set_data = ['ab_lq','ab_hq','all_lq','all_hq','all_aug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General config of the training run (!)\n",
    "\n",
    "# List of data to use for training\n",
    "\n",
    "# All possible train sets\n",
    "# train_sets = ['orig','plus','reord','phrase','dict','wordnet','ppdb','glove','fasttext','ab_lq','ab_hq','all_lq','all_hq']\n",
    "\n",
    "# Selective train set\n",
    "#train_sets = ['reord','phrase','dict']\n",
    "train_sets = ['wordnet','ppdb','glove','fasttext']\n",
    "#train_sets = ['wp1']\n",
    "\n",
    "# Alias path to aug data (either 125 or 500 examples or the 1500 joint)\n",
    "aug_path = path_to_aug_joint\n",
    "\n",
    "# Training parameters\n",
    "# Number of folds for k-fold cross validation\n",
    "n_k_fold = 10\n",
    "\n",
    "# Only answers (False) or questions + answers (True)\n",
    "mind_qa = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined dataset (original + augmented) for training and testing\n",
    "combined_data = load_combined_data()\n",
    "\n",
    "\n",
    "\n",
    "# # Get the datasets in dataframes\n",
    "# datasets = {}\n",
    "\n",
    "# # Check if we load only answers or questions plus answers\n",
    "# if mind_qa:\n",
    "#     # Always load MIND-CA + human aug, this is the base set\n",
    "#     datasets['plus'] = mr_get_qa_data(path_to_plus_files)\n",
    "\n",
    "#     # Always load UK-MIND-20, we need it for testing\n",
    "#     datasets['wp1'] = mr_get_qa_data(path_to_wp1)\n",
    "\n",
    "#     # If comparison is needed, load MIND-CA without any aug\n",
    "#     if 'orig' in train_sets:\n",
    "#         datasets['orig'] = mr_get_qa_data(path_to_raw_files)\n",
    "\n",
    "#     # Load augmented data\n",
    "#     for at_set in train_sets:\n",
    "#         if at_set in ['orig','plus','wp1']:\n",
    "#             continue\n",
    "#         path_to_aug = aug_path + at_set + \"/\"\n",
    "#         datasets[at_set] = mr_get_qa_data(path_to_aug)\n",
    "\n",
    "    \n",
    "# # Only the answer\n",
    "# else:\n",
    "#     # Always load MIND-CA + human aug, this is the base set\n",
    "#     datasets['plus'] = mr_get_data(path_to_plus_files)\n",
    "\n",
    "#     # Always load UK-MIND-20, we need it for testing\n",
    "#     datasets['wp1'] = mr_get_data(path_to_wp1)\n",
    "\n",
    "#     # If comparison is needed, load MIND-CA without any aug\n",
    "#     if 'orig' in train_sets:\n",
    "#         datasets['orig'] = mr_get_data(path_to_raw_files)\n",
    "\n",
    "#     # Load augmented data\n",
    "#     aug_dataset = {}\n",
    "#     for at_set in train_sets:\n",
    "#         if at_set in ['orig','plus','wp1']:\n",
    "#             continue\n",
    "#         path_to_aug = aug_path + at_set + \"/\"\n",
    "#         datasets[at_set] = mr_get_data(path_to_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check\n",
    "# for d_id in train_sets:\n",
    "#     print(len(datasets[d_id][-1][1]))\n",
    "#     if at_set in ['orig','plus','wp1']:\n",
    "#          continue\n",
    "#     # Augmented datasets have additional column that needs to be dropped\n",
    "#     datasets[d_id][-1][1].drop([\"Aug_ID\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr_proc_results(raw_results):\n",
    "\n",
    "  pr_results = [[[acc_score, f1_score,round(sum(qf_s)/11,2)],[acc_score_wp1, f1_score_wp1,round(sum(qf_s_wp1)/11,2)]] \n",
    "                for ([acc_score, qa_s, aa_s], [f1_score, qf_s, af_s],\n",
    "                     [acc_score_wp1, qa_s_wp1, aa_s_wp1], [f1_score_wp1, qf_s_wp1, af_s_wp1]) in raw_results]\n",
    "\n",
    "  # Throw the list in an np array\n",
    "  pr_arr = np.array(pr_results)\n",
    "\n",
    "  # Print the results\n",
    "  pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "  pp.pprint(pr_results)\n",
    "  pp.pprint(np.mean(pr_arr,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "if mind_qa:\n",
    "  # ages 8 to 13, removing outliers; allowed classes 0,1,2; max len 35\n",
    "  tr_cls = MR_transformer(text_cols,[8,9,10,11,12,13],[0,1,2],35)\n",
    "else:\n",
    "  # ages 8 to 13, removing outliers; allowed classes 0,1,2; max len 20\n",
    "  tr_cls = MR_transformer(text_cols,[8,9,10,11,12,13],[0,1,2],20)\n",
    "\n",
    "\n",
    "tr_cls.mr_set_eval_vars(True,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results variable \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data into training and validation sets\n",
    "train_df, test_df = train_test_split(combined_data, test_size=0.2)\n",
    "\n",
    "# Use k-fold cross-validation on the combined dataset\n",
    "results = tr_cls.mr_kfold_pre_split(train_df, test_df, 0.25, n_k_fold)\n",
    "\n",
    "# Save the results for the combined dataset\n",
    "rs_path = 'Results/split_eval_joint/tr_qa_combined.txt'\n",
    "with open(rs_path, 'w') as op:\n",
    "    simplejson.dump(results, op)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results for the combined dataset\n",
    "print(\"Combined dataset results:\")\n",
    "mr_proc_results(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
