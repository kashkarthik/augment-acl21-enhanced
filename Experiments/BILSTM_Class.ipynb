{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import glob\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn import *\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import gensim.models.wrappers.fasttext\n",
    "from scipy import sparse\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import LeaveOneOut,KFold,train_test_split\n",
    "import simplejson\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from mr_generic_scripts import *\n",
    "from mr_cls_BILSTM import *\n",
    "from mr_generic_scripts import load_combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to xlsx files folder\n",
    "\n",
    "# Original MIND-CA corpus\n",
    "path_to_raw_files = f_path + 'Data/raw_relabeled/'\n",
    "# MIND-CA + human augment\n",
    "path_to_plus_files = f_path + 'Data/raw_plus/'\n",
    "# UK-MIND-20\n",
    "path_to_wp1 = f_path + 'Data/wp1/'\n",
    "\n",
    "# Augmented data\n",
    "# augmentations, 125 examples per QA pair\n",
    "path_to_aug = f_path + 'Data/aug_data/all/'\n",
    "# augmentations, 500 examples per QA pair\n",
    "path_to_aug_hq_os = f_path + 'Data/aug_data_os/all/'\n",
    "# augmentation, no sampling - 1500 total examples per question\n",
    "path_to_aug_joint = f_path + 'Data/aug_data_joint/all/'\n",
    "\n",
    "# Merged xlsx files with multiple augmentations\n",
    "path_to_set_files = f_path + 'Data/aug_data/sets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of augmentations by category\n",
    "hq_data = ['reord','phrase','dict']\n",
    "lq_data = ['wordnet','ppdb','glove','fasttext']\n",
    "set_data = ['ab_lq','ab_hq','all_lq','all_hq','all_aug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General config of the training run (!)\n",
    "\n",
    "# List of data to use for training\n",
    "\n",
    "# All possible train sets\n",
    "# train_sets = ['orig','plus','reord','phrase','dict','wordnet','ppdb','glove','fasttext','ab_lq','ab_hq','all_lq','all_hq']\n",
    "\n",
    "# Selective train set\n",
    "#train_sets = ['reord','phrase','dict']\n",
    "train_sets = ['wordnet','ppdb','glove','fasttext']\n",
    "#train_sets = ['wp1']\n",
    "\n",
    "# Alias path to aug data (either 125 or 500 examples or the 1500 joint)\n",
    "aug_path = path_to_aug_joint\n",
    "\n",
    "# Training parameters\n",
    "# Number of folds for k-fold cross validation\n",
    "n_k_fold = 10\n",
    "\n",
    "# Only answers (False) or questions + answers (True)\n",
    "mind_qa = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the datasets in dataframes\n",
    "combined_data = load_combined_data{}\n",
    "\n",
    "# Check if we load only answers or questions plus answers\n",
    "if mind_qa:\n",
    "    # Always load MIND-CA + human aug, this is the base set\n",
    "    datasets['plus'] = mr_get_qa_data(path_to_plus_files)\n",
    "\n",
    "    # Always load UK-MIND-20, we need it for testing\n",
    "    datasets['wp1'] = mr_get_qa_data(path_to_wp1)\n",
    "\n",
    "    # If comparison is needed, load MIND-CA without any aug\n",
    "    if 'orig' in train_sets:\n",
    "        datasets['orig'] = mr_get_qa_data(path_to_raw_files)\n",
    "\n",
    "    # Load augmented data\n",
    "    for at_set in train_sets:\n",
    "        if at_set in ['orig','plus','wp1']:\n",
    "            continue\n",
    "        path_to_aug = aug_path + at_set + \"/\"\n",
    "        datasets[at_set] = mr_get_qa_data(path_to_aug)\n",
    "\n",
    "    \n",
    "# Only the answer\n",
    "else:\n",
    "    # Always load MIND-CA + human aug, this is the base set\n",
    "    datasets['plus'] = mr_get_data(path_to_plus_files)\n",
    "\n",
    "    # Always load UK-MIND-20, we need it for testing\n",
    "    datasets['wp1'] = mr_get_data(path_to_wp1)\n",
    "\n",
    "    # If comparison is needed, load MIND-CA without any aug\n",
    "    if 'orig' in train_sets:\n",
    "        datasets['orig'] = mr_get_data(path_to_raw_files)\n",
    "\n",
    "    # Load augmented data\n",
    "    aug_dataset = {}\n",
    "    for at_set in train_sets:\n",
    "        if at_set in ['orig','plus','wp1']:\n",
    "            continue\n",
    "        path_to_aug = aug_path + at_set + \"/\"\n",
    "        datasets[at_set] = mr_get_data(path_to_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for d_id in train_sets:\n",
    "    print(len(datasets[d_id][-1][1]))\n",
    "    if at_set in ['orig','plus','wp1']:\n",
    "         continue\n",
    "    # Augmented datasets have additional column that needs to be dropped\n",
    "    datasets[d_id][-1][1].drop([\"Aug_ID\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr_proc_results(raw_results):\n",
    "  # Process the results from the 10 runs\n",
    "  # result format: [acc, acc per q, acc per age], [f1, f1 per q, f1 per age], [acc, acc per q, acc per age] (for wp1), [f1, f1 per q, f1 per age] (for wp1)\n",
    "  # Ignore ages as they seem to be mostly consistent with global average\n",
    "  # Ignore accs per question and age as averaging them seems to be consistent with global average\n",
    "  # Report global acc, global macro f1, average of macro f1 per question; same for wp1\n",
    "  pr_results = [[[acc_score, f1_score,round(sum(qf_s)/11,2)],[acc_score_wp1, f1_score_wp1,round(sum(qf_s_wp1)/11,2)]] \n",
    "                for ([acc_score, qa_s, aa_s], [f1_score, qf_s, af_s],\n",
    "                     [acc_score_wp1, qa_s_wp1, aa_s_wp1], [f1_score_wp1, qf_s_wp1, af_s_wp1]) in raw_results]\n",
    "\n",
    "  # Throw the list in an np array\n",
    "  pr_arr = np.array(pr_results)\n",
    "\n",
    "  # Print the results\n",
    "  pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "  pp.pprint(pr_results)\n",
    "  pp.pprint(np.mean(pr_arr,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "if mind_qa:\n",
    "  # ages 8 to 13, removing outliers; vocabulary of 1000; max len 35\n",
    "  bl_cls = MR_bilstm(text_cols,[7,8,9,10,11,12,13,14],1000,35)\n",
    "else:\n",
    "  # ages 8 to 13, removing outliers; vocabulary of 1000; max len 20\n",
    "  bl_cls = MR_bilstm(text_cols,[7,8,9,10,11,12,13,14],1000,20)\n",
    "\n",
    "# Configure eval parameters - eval by age and questions, do not return examples with errors (not fully implemented in current version)  \n",
    "bl_cls.mr_set_eval_vars(True,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results variable \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the combined dataset for training and evaluation\n",
    "train_df, test_df = train_test_split(combined_data, test_size=0.2)\n",
    "\n",
    "# Train and evaluate using k-fold cross-validation\n",
    "results['combined'] = bl_cls.mr_kfold_pre_split(train_df, test_df, 0.25, n_k_fold)\n",
    "\n",
    "# Save the results\n",
    "rs_path = 'Results/split_eval_joint/bl_qa_combined.txt'\n",
    "with open(rs_path, 'w') as op:\n",
    "    simplejson.dump(results['combined'], op)\n",
    "\n",
    "\n",
    "# Run all train-test combos\n",
    "# for at_set in train_sets:\n",
    "#     print(\"Current train: \" + str(at_set) + \"\\n\")\n",
    "\n",
    "#     if at_set in ['orig','plus','wp1']:\n",
    "#         # For orig and plus we directly train and test using kfold validation\n",
    "#         results[at_set] = bl_cls.mr_kfold_pre_split(datasets[at_set][-1][1],datasets['wp1'][-1][1],0.25,n_k_fold)\n",
    "#     else:\n",
    "#         # For augmented data we need to also provide the \"plus\" set for evaluation and organizing the split\n",
    "#         results[at_set] = bl_cls.mr_kfold_aug_pre_split(datasets['plus'][-1][1],datasets[at_set][-1][1],datasets['wp1'][-1][1],0.25,n_k_fold)\n",
    "\n",
    "#     # Save the results in a file\n",
    "#     rs_path = 'Results/split_eval_joint/bl_qa_os_'\n",
    "#     s_path = rs_path + at_set + '.txt'\n",
    "#     with open(s_path,'w') as op:\n",
    "#         simplejson.dump(results[at_set],op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "# Visualize the results for the combined dataset\n",
    "print(\"Combined dataset results:\")\n",
    "mr_proc_results(results['combined'])\n",
    "\n",
    "\n",
    "# for at_set in train_sets:\n",
    "#     print(at_set)\n",
    "#     mr_proc_results(results[at_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
